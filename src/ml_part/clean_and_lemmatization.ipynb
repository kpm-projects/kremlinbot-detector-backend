{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "!pip install pymorphy2\n",
    "!pip install pymystem3\n",
    "# !pip install git+https://github.com/nlpub/pymystem3\n",
    "# !pip install -q http://download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz\n",
    "from pymystem3 import Mystem\n",
    "import pymorphy2\n",
    "from string import punctuation\n",
    "\n",
    "!pip install pandarallel\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "%cd /content/drive/My\\ Drive/SNA2020/Project\n",
    "\n",
    "df_bots=pd.read_csv('./bots_update.tsv',sep=\"\\t\",header=0)\n",
    "df_bots.head()\n",
    "\n",
    "df_politics = pd.read_csv('./politics_update.tsv',sep=\"\\t\",header=0)\n",
    "df_bots.head()\n",
    "\n",
    "url_stopwords_ru = \"https://raw.githubusercontent.com/stopwords-iso/stopwords-ru/master/stopwords-ru.txt\"\n",
    "\n",
    "badwords = [\n",
    "  u'я', u'а', u'да', u'но', u'тебе', u'мне', u'ты', u'и', u'у', u'на', u'ща', u'ага',\n",
    "  u'так', u'там', u'какие', u'который', u'какая', u'туда', u'давай', u'короче', u'кажется', u'вообще',\n",
    "  u'ну', u'не', u'чет', u'неа', u'свои', u'наше', u'хотя', u'такое', u'например', u'кароч', u'как-то',\n",
    "  u'нам', u'хм', u'всем', u'нет', u'да', u'оно', u'своем', u'про', u'вы', u'м', u'тд',\n",
    "  u'вся', u'кто-то', u'что-то', u'вам', u'это', u'эта', u'эти', u'этот', u'прям', u'либо', u'как', u'мы',\n",
    "  u'просто', u'блин', u'очень', u'самые', u'твоем', u'ваша', u'кстати', u'вроде', u'типа', u'пока', u'ок'\n",
    "]\n",
    "\n",
    "emoticons = [u'&#_128514;', u'&#_128517;', u'u&#34;\\U0001F923&#34;']\n",
    "\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U0001F923\"\n",
    "        u\"\\U0001F9A0\"\n",
    "        u\"\\U0001F000-\\U0001FFFF\"\n",
    "        u\"\\U00002639\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "def get_text(url, encoding='utf-8', to_lower=True):\n",
    "    url = str(url)\n",
    "    if url.startswith('http'):\n",
    "        r = requests.get(url)\n",
    "        if not r.ok:\n",
    "            r.raise_for_status()\n",
    "        return r.text.lower() if to_lower else r.text\n",
    "    elif os.path.exists(url):\n",
    "        with open(url, encoding=encoding) as f:\n",
    "            return f.read().lower() if to_lower else f.read()\n",
    "    else:\n",
    "        raise Exception('parameter [url] can be either URL or a filename')\n",
    "\n",
    "def preprocess_text_mention_and_n(text):\n",
    "  if text.find(\" \") - 1 > 0 and text[text.find(\" \") - 1] == ',':\n",
    "    text = text[text.find(\" \") + 1:]\n",
    "  text = text.replace('\\\\n', ' ')\n",
    "  return text\n",
    "\n",
    "def clean_emoticons(text):\n",
    "  for word in emoticons:\n",
    "    text = text.replace(word, '')\n",
    "  text = emoji_pattern.sub(r'', text)\n",
    "  return text\n",
    "\n",
    "def preprocess_text_lemmatize(text, setting=\"pymorphy\"):\n",
    "    text = text.lower()\n",
    "    if setting == \"mystem\":\n",
    "      mystem = Mystem()\n",
    "      tokens = mystem.lemmatize(text)\n",
    "    elif setting == \"pymorphy\":\n",
    "      tokens = word_tokenize(text, language=\"russian\")\n",
    "      morph = pymorphy2.MorphAnalyzer()\n",
    "      tokens = [morph.parse(token)[0].normal_form for token in tokens]\n",
    "    else:\n",
    "       raise Exception('parameter setting should be fill')\n",
    "    tokens = [token for token in tokens if token.strip() not in punctuation]\n",
    "    tokens = \" \".join(tokens)\n",
    "    tokens = tokens.replace(\"``\", '').replace(\"''\", '').replace(\".\", '').replace(\"«\", '').replace(\"»\", '').replace(\"—\", '').replace(\"№\", '')\n",
    "    for symbol in punctuation:\n",
    "      tokens = tokens.replace(symbol, '')\n",
    "    for symbol in ['1', '2', '3', '4', '5', '6', '7', '8', '9', '0']:\n",
    "      tokens = tokens.replace(symbol, '')\n",
    "    # print(tokens)\n",
    "    return tokens\n",
    "\n",
    "def preprocess_text_stopwords(tokens, setting):\n",
    "    if setting == \"nltk-stopwords\":\n",
    "      stopwords = stopwords.words(\"russian\")\n",
    "    elif setting == \"github-stopwords\":\n",
    "      stopwords = get_text(url_stopwords_ru).splitlines()\n",
    "    elif setting == \"badwords\":\n",
    "      stopwords = badwords\n",
    "    else:\n",
    "       raise Exception('parameter setting should be fill')\n",
    "    \n",
    "    tokens = [token for token in tokens if token not in stopwords \\\n",
    "              and len(token) > 2 \\\n",
    "              and token != \" \" \\\n",
    "              and token.strip() not in punctuation]\n",
    "    return tokens\n",
    "\n",
    "pandarallel.initialize()\n",
    "\n",
    "# df_bots['text']=df_bots['text'].parallel_apply(preprocess_text_mention_and_n)\n",
    "# df_bots['text']=df_bots['text'].parallel_apply(clean_emoticons)\n",
    "# df_bots['pymorphy']=df_bots['text'].parallel_apply(preprocess_text_lemmatize)\n",
    "# print(df_bots['pymorphy'][100:200])\n",
    "# df_bots.to_csv('bots_pymorphy.tsv',sep='\\t', quoting=csv.QUOTE_NONE)\n",
    "\n",
    "df_politics['text']=df_politics['text'].parallel_apply(preprocess_text_mention_and_n)\n",
    "df_politics['text']=df_politics['text'].parallel_apply(clean_emoticons)\n",
    "df_politics['pymorphy']=df_politics['text'].parallel_apply(preprocess_text_lemmatize)\n",
    "print(df_politics['pymorphy'][100:200])\n",
    "df_politics.to_csv('politics_pymorphy.tsv',sep='\\t', quoting=csv.QUOTE_NONE)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}