{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install pandarallel\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.pipeline import FeatureUnion,Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "\n",
    "\n",
    "%cd /content/drive/My\\ Drive/SNA2020/Project_kremlinbots\n",
    "\n",
    "def metrics(clf, X, y):\n",
    "  scores = cross_validate(clf, X, y, cv=5, scoring=['accuracy','f1_macro'])\n",
    "  res = [scores['test_accuracy'].mean(), scores['test_f1_macro'].mean()]\n",
    "  print(res)\n",
    "\n",
    "def learning(X, y, mapper, min_max_scaler, name):\n",
    "  print(\"LinearSVC()\")\n",
    "  metrics(LinearSVC(), X, y)\n",
    "  print(\"RandomForestClassifier()\")\n",
    "  metrics(RandomForestClassifier(n_estimators=300), X, y)\n",
    "  model = RandomForestClassifier(n_estimators=300)\n",
    "  model.fit(X, y)\n",
    "  pkl_filename = name + \"_RandomForestClassifier_model.pkl\"\n",
    "  tuple_objects = (model, mapper, min_max_scaler)\n",
    "  with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(tuple_objects, file)\n",
    "  print(\"GaussianNB()\")\n",
    "  metrics(GaussianNB(), X, y)\n",
    "  print(\"AdaBoostClassifier()\")\n",
    "  metrics(AdaBoostClassifier(), X, y)\n",
    "  print(\"LogisticRegression()\")\n",
    "  metrics(LogisticRegression(), X, y)\n",
    "\n",
    "def processing(df_all, name):\n",
    "  print(name)\n",
    "  mapper = DataFrameMapper([\n",
    "     (name, text.TfidfVectorizer(max_features=1400)),\n",
    "     ('has_media', None),\n",
    "     ('time_dif', None),\n",
    "     ('likes_cnt', None),\n",
    "     ('self_like', None),\n",
    "     ('replies_cnt', None),\n",
    "     ('reg_date_dif', None),\n",
    "     ('is_closed', None),\n",
    "  ])\n",
    "  y = df_all['is_bot']\n",
    "  X = mapper.fit_transform(df_all)\n",
    "  min_max_scaler = preprocessing.MinMaxScaler()\n",
    "  np_scaled = min_max_scaler.fit_transform(X)\n",
    "  X = pd.DataFrame(np_scaled) \n",
    "  learning(X, y, mapper, min_max_scaler, name)\n",
    "  print(X)\n",
    "  print(\"_______________________________________\")\n",
    "\n",
    "df_all_pymorphy_nltk_stopwords_badwords=pd.read_csv('./all_pymorphy_nltk-stopwords_badwords.tsv',sep=\"\\t\",header=0)\n",
    "processing(df_all_pymorphy_nltk_stopwords_badwords, \"pymorphy_nltk-stopwords_badwords\")\n",
    "df_all_pymorphy_github_stopwords_badwords=pd.read_csv('./all_pymorphy_github-stopwords_badwords.tsv',sep=\"\\t\",header=0)\n",
    "processing(df_all_pymorphy_github_stopwords_badwords, \"pymorphy_github-stopwords_badwords\")\n",
    "df_all_pymorphy_nltk_stopwords_github_stopwords_badwords=pd.read_csv('./all_pymorphy_nltk-stopwords_github-stopwords_badwords.tsv',sep=\"\\t\",header=0)\n",
    "processing(df_all_pymorphy_nltk_stopwords_github_stopwords_badwords, \"pymorphy_nltk-stopwords_github-stopwords_badwords\")\n",
    "\n",
    "df_all_mystem_nltk_stopwords_badwords=pd.read_csv('./all_mystem_nltk-stopwords_badwords.tsv',sep=\"\\t\",header=0)\n",
    "processing(df_all_mystem_nltk_stopwords_badwords, \"mystem_nltk-stopwords_badwords\")\n",
    "df_all_mystem_github_stopwords_badwords=pd.read_csv('./all_mystem_github-stopwords_badwords.tsv',sep=\"\\t\",header=0)\n",
    "processing(df_all_mystem_github_stopwords_badwords, \"mystem_github-stopwords_badwords\")\n",
    "df_all_mystem_nltk_stopwords_github_stopwords_badwords=pd.read_csv('./all_mystem_nltk-stopwords_github-stopwords_badwords.tsv',sep=\"\\t\",header=0)\n",
    "processing(df_all_mystem_nltk_stopwords_github_stopwords_badwords, \"mystem_nltk-stopwords_github-stopwords_badwords\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}