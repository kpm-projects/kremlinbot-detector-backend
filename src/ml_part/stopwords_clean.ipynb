{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "import requests\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"stopwords\")\n",
    "import copy\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "\n",
    "!pip install pandarallel\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "%cd /content/drive/My\\ Drive/SNA2020/Project\n",
    "\n",
    "df_bots_pymorphy=pd.read_csv('./bots_pymorphy.tsv',sep=\"\\t\",header=0)\n",
    "df_bots_pymorphy.head()\n",
    "\n",
    "df_politics_pymorphy=pd.read_csv('./politics_pymorphy.tsv',sep=\"\\t\",header=0)\n",
    "df_politics_pymorphy.head()\n",
    "\n",
    "url_stopwords_ru = \"https://raw.githubusercontent.com/stopwords-iso/stopwords-ru/master/stopwords-ru.txt\"\n",
    "\n",
    "badwords = [\n",
    "  u'я', u'а', u'да', u'но', u'тебе', u'мне', u'ты', u'и', u'у', u'на', u'ща', u'ага',\n",
    "  u'так', u'там', u'какие', u'который', u'какая', u'туда', u'давай', u'короче', u'кажется', u'вообще',\n",
    "  u'ну', u'не', u'чет', u'неа', u'свои', u'наше', u'хотя', u'такое', u'например', u'кароч', u'как-то',\n",
    "  u'нам', u'хм', u'всем', u'нет', u'да', u'оно', u'своем', u'про', u'вы', u'м', u'тд',\n",
    "  u'вся', u'кто-то', u'что-то', u'вам', u'это', u'эта', u'эти', u'этот', u'прям', u'либо', u'как', u'мы',\n",
    "  u'просто', u'блин', u'очень', u'самые', u'твоем', u'ваша', u'кстати', u'вроде', u'типа', u'пока', u'ок'\n",
    "]\n",
    "\n",
    "\n",
    "def get_text(url, encoding='utf-8', to_lower=True):\n",
    "    url = str(url)\n",
    "    if url.startswith('http'):\n",
    "        r = requests.get(url)\n",
    "        if not r.ok:\n",
    "            r.raise_for_status()\n",
    "        return r.text.lower() if to_lower else r.text\n",
    "    elif os.path.exists(url):\n",
    "        with open(url, encoding=encoding) as f:\n",
    "            return f.read().lower() if to_lower else f.read()\n",
    "    else:\n",
    "        raise Exception('parameter [url] can be either URL or a filename')\n",
    "\n",
    "github_stopwords = get_text(url_stopwords_ru).splitlines()\n",
    "\n",
    "def preprocess_text_stopwords(text, setting=\"nltk-stopwords\"):\n",
    "    # print(text)\n",
    "    tokens = text.split()\n",
    "    if setting == \"nltk-stopwords\":\n",
    "      stopwords_for_delete = stopwords.words(\"russian\")\n",
    "    elif setting == \"github-stopwords\":\n",
    "      stopwords_for_delete = github_stopwords\n",
    "    elif setting == \"badwords\":\n",
    "      stopwords_for_delete = badwords\n",
    "    else:\n",
    "       raise Exception('parameter setting should be fill')\n",
    "    \n",
    "    tokens = [token for token in tokens if token not in stopwords_for_delete \\\n",
    "              and len(token) > 2 \\\n",
    "              and token != \" \" \\\n",
    "              and token.strip() not in punctuation]\n",
    "    tokens = \" \".join(tokens)\n",
    "    return tokens\n",
    "\n",
    "pandarallel.initialize()\n",
    "\n",
    "def make_output_for_one_stopword_list(base_df, name_of_people, name_of_lemm, name_of_stopword):\n",
    "  # after_stopwords = base_df.copy(deep=True)\n",
    "  after_stopwords = copy.deepcopy(base_df)\n",
    "  after_stopwords[name_of_lemm + '_' + name_of_stopword]=base_df[name_of_lemm].parallel_apply(preprocess_text_stopwords,setting=name_of_stopword)\n",
    "  after_stopwords[name_of_lemm + '_' + name_of_stopword].replace('', np.nan, inplace=True)\n",
    "  after_stopwords = after_stopwords.dropna()\n",
    "  after_stopwords.to_csv(name_of_people + '_' + name_of_lemm + '_' + name_of_stopword + '.tsv',sep='\\t', quoting=csv.QUOTE_NONE)\n",
    "  return after_stopwords\n",
    "\n",
    "def make_output_for_many_stopword_lists(base_df, name_of_people, name_of_lemm, name_of_stopword, name_of_used_stopword):\n",
    "  print(type(base_df))\n",
    "  # after_stopwords = base_df.copy(deep=True)\n",
    "  after_stopwords = copy.deepcopy(base_df)\n",
    "  after_stopwords[name_of_lemm + '_' + name_of_stopword + '_' + name_of_used_stopword]=base_df[name_of_lemm + '_' + name_of_used_stopword].parallel_apply(preprocess_text_stopwords,setting=name_of_stopword)\n",
    "  after_stopwords[name_of_lemm + '_' + name_of_stopword + '_' + name_of_used_stopword].replace('', np.nan, inplace=True)\n",
    "  after_stopwords = after_stopwords.dropna()\n",
    "  after_stopwords.to_csv(name_of_people + '_' + name_of_lemm + '_' + name_of_stopword + '_' + name_of_used_stopword + '.tsv',sep='\\t', quoting=csv.QUOTE_NONE)\n",
    "  return after_stopwords\n",
    "\n",
    "\n",
    "def make_output_with_stopwords(base_df, name_of_people, name_of_lemm):\n",
    "  base_df = base_df.dropna()\n",
    "  \n",
    "  nltk_stopwords = make_output_for_one_stopword_list(base_df, name_of_people, name_of_lemm, \"nltk-stopwords\")\n",
    "  github_stopwords = make_output_for_one_stopword_list(base_df, name_of_people, name_of_lemm, \"github-stopwords\")\n",
    "  badwords_stopwords = make_output_for_one_stopword_list(base_df, name_of_people, name_of_lemm, \"badwords\")\n",
    "\n",
    "  print('end_first_part')\n",
    "\n",
    "  nltk_stopwords_github_stopwords = make_output_for_many_stopword_lists(github_stopwords, name_of_people, name_of_lemm, \"nltk-stopwords\", \"github-stopwords\")\n",
    "  print(\"nltk_stopwords_github_stopwords\")\n",
    "  nltk_stopwords_badwords = make_output_for_many_stopword_lists(badwords_stopwords, name_of_people, name_of_lemm, \"nltk-stopwords\", \"badwords\")\n",
    "\n",
    "  github_stopwords_badwords = make_output_for_many_stopword_lists(badwords_stopwords, name_of_people, name_of_lemm, \"github-stopwords\", \"badwords\")\n",
    "\n",
    "  nltk_stopwords_github_stopwords_badwords = make_output_for_many_stopword_lists(github_stopwords_badwords, name_of_people, name_of_lemm, \"nltk-stopwords\", \"github-stopwords_badwords\")\n",
    "\n",
    "\n",
    "# make_output_with_stopwords(df_bots_pymorphy, \"bots\", \"pymorphy\")\n",
    "\n",
    "make_output_with_stopwords(df_politics_pymorphy, \"politics\", \"pymorphy\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}